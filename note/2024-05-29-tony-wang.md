## Todos

1. Come up with better way to share/take notes
2. Talk with Rylan Schaeffer — rylanschaeffer@gmail.com.
3. Pester Ethan for access to unfettered access to 4o.
	1. Ask Tony for temp API key if not possible.

## Notes

1. Papers that do automated red-teaming
	1. LLM-agents
		1. https://arxiv.org/abs/2310.08419
		2. https://arxiv.org/abs/2312.02119
		3. https://blog.haizelabs.com/posts/dspy/
	2. Gradient-based attacks
		1. https://blog.haizelabs.com/posts/acg/
	3. Black-box / grey-box image attacks literature
		1. Yang to find
	4. Yang recs from EP
		1. https://arxiv.org/abs/2308.04265
		2. https://arxiv.org/abs/2202.03286

## Project description

1. Automated red-teaming for multi-modal models
	1. What types of behaviors are we trying to exhibit?
		1. Data leakage/user info
		2. Harmful responses
		3. Distributional bias? (Responding differently/more harmful to certain substitutable prompts)
2. Black-box vs. grey-box (logits) vs. white-box?
	1. To ask EP
	2. Yang: we can probably work down from white-box starters.
3. Red->Text->Image->MM pipeline is accessible.
	1. Red->Image->MM possible? How?
		1. Clip-esque ad.ex. Work to look into
		2. context
	2. Audio? Yikes
	3. OOD vs ID inputs consideration, which one is more accessible vs. interesting

## Tony’s idea

1. Set the target as gpt-4o.
2. Possible goal 1: Try to elicit bad text-outputs when you also have the ability to input arbitrary images as input.
	1. Can use this dataset of bad behaviors as a target: https://github.com/centerforaisafety/HarmBench/tree/main/data
3. Possible goal 2: Try to elicit bad outputs that take into consideration the image context.
	1. Can use the multimodal dataset of behaviors at https://github.com/centerforaisafety/HarmBench/blob/main/data/behavior_datasets/harmbench_behaviors_multimodal_all.csv
4. **For each of these goals, the idea is to try to beat existing attacks when you can take advantage of additional modalities.**
	1. You can again compare to how strong existing attacks are at https://www.harmbench.org/results.
