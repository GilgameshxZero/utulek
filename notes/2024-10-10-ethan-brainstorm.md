
## Jan Leike

### Unsupervised honesty: <https://docs.google.com/document/d/1IG17JWnR1TcuZgM5QQhJ09yjmZIeqfWbLjKj18m7GiE/edit?tab=t.0>

Ryan had a cool idea about geometrically placing activation vectors—perhaps orthogonal beliefs have orthogonal activation vectors at some layer.

## Buck Shlegeris

### Agent benchmarks: <https://docs.google.com/document/d/1JfC18jn4TXaM87vaxy0Bqf3bOHPGHhV_G7DdMG5y_N4/edit?tab=t.0#heading=h.ecm7meyctpj0>.

### Control with training: <https://docs.google.com/document/d/1fomC01yLKQkDyZiy9lB64pXZ57aG6fdgzcu1vlFrAv4/edit?tab=t.0>

1. Try best effort, but the conclusion is probably that this is not very effective without a scratchpad.

### Control for auditing: <https://docs.google.com/document/d/1JMfbF00hTD8_fv1_NyyUcnJlpMJJqv1avgs27QCrRQs/edit?tab=t.0>

1. I should probably read the more general unsupervised auditing paper that Buck mentioned he had written up before.
2. First pass collusion is obvious with a small comment in the original actor to trigger some PL-behavior in the monitor.

## Erik Jones: <https://docs.google.com/document/d/1h4g2sB-zE7BlwcqioFFZQ0eI7pBc7OHV-Q1lwOa0G0M/edit?tab=t.0>.

### MA safety

1. Super interesting personally.
2. What are examples of MA collusion that we can get first-iteration?
3. Need a good use case to ground this project imo.
4. Multiple

## Julian Stastny

### Plan B: <https://docs.google.com/document/d/1hsK34H4kedc_7eVLne4GtyOTKeLw2YmtIiTeAeUpxII/edit?tab=t.0>

### Adapters for control: <https://docs.google.com/document/d/1po7Btupr7-4wzePVT1WOdak3nOZtO20u9N2crXMyzRA/edit?tab=t.0>

1. Pretty interesting idea.

## John Hughes

### BoN next steps: <https://docs.google.com/document/d/1FVfZ6xHnuf8z2CJsQNBQlS4H3_Pz6y_F2Wskne0zIMk/edit?tab=t.0#heading=h.7b9ju1exp38x>

1. Ngl I don’t understand augmentations too well, I probably need to read Sara, John, and Aengus’s work.

## Sam

1. This is just big trans.

## Fabien

1. Isn’t PL part of finetuning robustness as well?
<https://docs.google.com/document/d/1hlrMYGBEZPwW8Aqa2u3MA3PurYg2h6A7xBND91VAg9U/edit?tab=t.0>
<https://docs.google.com/document/d/19Y1OpDWlSn7Mbl96emcO7XWx5R9hVO26KwKaMnBzppM/edit?tab=t.0#heading=h.u0pgexrzrruw>

## Jiaxin

<https://docs.google.com/document/d/1wwWpX9VXzl6PUeyud8xfeGIwfNwweyWloc0fsZxALRg/edit?tab=t.0#heading=h.1u3x6hbzgopq>.

## Yang Yan

1. Maybe finetuning can be improved by selectively step-wise finetuning on different things.
